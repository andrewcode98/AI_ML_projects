{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deffcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Data_Processing import preprocessing, modify_target_binary\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2c6786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_12080\\2285443003.py:1: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  loan_df = pd.read_csv(\"data/accepted_2007_to_2018Q4.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame read!\n"
     ]
    }
   ],
   "source": [
    "loan_df = pd.read_csv(\"data/accepted_2007_to_2018Q4.csv\")\n",
    "print(\"DataFrame read!\")\n",
    "loan_df = modify_target_binary(loan_df, \"loan_status\")\n",
    "# Remove Current/Issued targets before preprocessing\n",
    "loan_df = loan_df.loc[~loan_df[\"loan_status\"].isin([\"Current\",\"Issued\"])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe06e",
   "metadata": {},
   "source": [
    "Training using class weights and Random Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c52e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [32:59<00:00, 39.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV F1-score: 0.7172723166024167\n",
      "{'n_estimators': np.int64(300), 'max_depth': np.int64(30), 'min_samples_split': np.int64(17), 'max_features': np.str_('sqrt'), 'min_samples_leaf': np.int64(2), 'class_weight': np.str_('balanced')}\n",
      "XGBoost :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [1:25:42<00:00, 102.85s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV F1-score: 0.7417546970210589\n",
      "{'n_estimators': np.int64(300), 'max_depth': np.int64(5), 'learning_rate': np.float64(0.1), 'objective': np.str_('binary:logistic'), 'subsample': np.float64(0.8), 'colsample_bytree': np.float64(0.1), 'reg_alpha': np.float64(0.5), 'reg_lambda': np.float64(0.1)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state= 42)\n",
    "RF_grid_dist =        {\"n_estimators\": [300],\n",
    "                       \"max_depth\" :  np.arange(5, 61, 5),\n",
    "                       \"min_samples_split\" : np.arange(2,21),\n",
    "                       \"max_features\": [\"sqrt\",\"log2\"],\n",
    "                       \"min_samples_leaf\": np.arange(2,21),\n",
    "                       \"class_weight\": [\"balanced\"],\n",
    "                       \"n_jobs\" : [-1]\n",
    "                       }\n",
    "\n",
    "XGBoost_grid_dist = {\"n_estimators\": np.array([300]),\n",
    "                     \"max_depth\": np.arange(5,61,5),\n",
    "                     \"learning_rate\":np.array([0.001, 0.01, 0.1, 0.5]),\n",
    "                     \"objective\": [\"binary:logistic\"],\n",
    "                     \"subsample\": np.arange(0.1, 1.0, 0.1),\n",
    "                     \"colsample_bytree\": np.arange(0.1, 1.0, 0.1),\n",
    "                     \"reg_alpha\": np.array([0.01, 0.05, 0.1, 0.5, 0.8]),\n",
    "                     \"reg_lambda\": np.array([0.01, 0.05, 0.1, 0.5, 0.8])}\n",
    "\n",
    "# Function to perform random search with stratified k-fold validation\n",
    "def RANDOM_SEARCH_CV_SS(param_dist: Dict, n_iterations: int, X_train: pd.DataFrame, y_train: pd.Series, model_classifier, cv:StratifiedKFold):\n",
    "    results = []\n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "\n",
    "    for _ in tqdm(range(n_iterations)):\n",
    "\n",
    "\n",
    "        params = {}\n",
    "        for key,values in param_dist.items():\n",
    "            params[key] = np.random.choice(values)\n",
    "\n",
    "        f1_scores = []\n",
    "        recalls = []\n",
    "        accuracy_scores = []\n",
    "\n",
    "        for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "            model = model_classifier(\n",
    "                **params\n",
    "            )\n",
    "\n",
    "            model.fit(X_tr, y_tr)\n",
    "            y_pred = model.predict(X_val)\n",
    "\n",
    "            f1_scores.append(f1_score(y_val, y_pred))\n",
    "            recalls.append(recall_score(y_val, y_pred))\n",
    "            accuracy_scores.append(accuracy_score(y_val, y_pred))\n",
    "\n",
    "        mean_f1 = np.mean(f1_scores)\n",
    "\n",
    "        if mean_f1 > best_score:\n",
    "            best_score = mean_f1\n",
    "            best_model = model\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score, best_model\n",
    "    \n",
    "# Function to tune random forest hyperparameters on sample of data\n",
    "def hyperparameter_tuning(rows_sample:int, df:pd.DataFrame, cv:StratifiedKFold, param_dist:Dict, model_classifier):\n",
    "    df_sample = df.sample(n = rows_sample)\n",
    "    X_sample =  df_sample.drop(columns=\"loan_status\")\n",
    "    y_sample =  df_sample[\"loan_status\"]\n",
    "    X_sample = preprocessing(X_sample)\n",
    "    best_params, best_score, best_model = RANDOM_SEARCH_CV_SS(param_dist, 50, X_sample, y_sample, model_classifier, cv)\n",
    "    print(f\"Best CV F1-score: {best_score}\")\n",
    "    print(best_params)\n",
    "    return best_params, best_score, best_model\n",
    "\n",
    "print(\"Random Forest: \")\n",
    "best_RF_params, best_f1_score, best_RF_model = hyperparameter_tuning(10000, loan_df, skf, RF_grid_dist, RandomForestClassifier)\n",
    "print(\"XGBoost :\")\n",
    "best_XGB_params, best_f1_score, best_XGB_model = hyperparameter_tuning(10000, loan_df, skf, XGBoost_grid_dist, XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8615e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing finished!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# XGBoost best params\u001b[39;00m\n\u001b[32m     12\u001b[39m XGB_model = XGBClassifier(**best_XGB_params)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mXGB_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mXBG model trained!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Random Forest best params\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\sklearn.py:1806\u001b[39m, in \u001b[36mXGBClassifier.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1786\u001b[39m evals_result: EvalsLog = {}\n\u001b[32m   1787\u001b[39m train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[32m   1788\u001b[39m     missing=\u001b[38;5;28mself\u001b[39m.missing,\n\u001b[32m   1789\u001b[39m     X=X,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1803\u001b[39m     feature_types=feature_types,\n\u001b[32m   1804\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1806\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1809\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1811\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.objective):\n\u001b[32m   1821\u001b[39m     \u001b[38;5;28mself\u001b[39m.objective = params[\u001b[33m\"\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:774\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    773\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m774\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:199\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:2434\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2433\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2434\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2435\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2436\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2439\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training using best hyperparameters on the whole dataset\n",
    "X = loan_df.drop(columns=[\"loan_status\"])\n",
    "y = loan_df[\"loan_status\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, \n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = True, \n",
    "                                                    stratify = y)\n",
    "\n",
    "X_train, X_test = preprocessing(X_train), preprocessing(X_test)\n",
    "print(\"Processing finished!\")\n",
    "# XGBoost best params\n",
    "XGB_model = XGBClassifier(**best_XGB_params)\n",
    "XGB_model.fit(X_train,y_train)\n",
    "print(\"XBG model trained!\")\n",
    "# Random Forest best params\n",
    "RF_model = RandomForestClassifier(**best_RF_params)\n",
    "RF_model.fit(X_train,y_train)\n",
    "print(\"Random Forest trained!\")\n",
    "# Save the models trained with the best hyperparameters\n",
    "XGB_model.save_model(\"xgb_model_1.json\")\n",
    "RF_model.save_model(\"rf_model_1.json\")\n",
    "print(\"Models saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
